{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d98d21d-3ad8-4a64-ac7d-ea907c56ab55",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc76ac-04a8-4b37-a45f-f2066cf26ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Scraping Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804df00-2437-44e6-94bf-558d48a06075",
   "metadata": {},
   "source": [
    "Mengambil 20000 data yang paling relevan dari aplikasi KAI Access Google Play Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77612db5-3288-49c2-841e-afc01258dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import Sort, reviews\n",
    "\n",
    "# Mengambil ulasan dari Aplikasi KAI Access\n",
    "app_id = 'com.kai.kaiticketing'\n",
    "result, continuation_token = reviews(\n",
    "    app_id,\n",
    "    lang='id',\n",
    "    country='id',\n",
    "    sort=Sort.MOST_RELEVANT,\n",
    "    filter_score_with=None,\n",
    "    count=20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2783b-3917-4738-8cb6-10ea77adbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# menampilkan hasil scraping\n",
    "df_scraping = pd.DataFrame(result)\n",
    "print(len(df_scraping))\n",
    "df_scraping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef65cd-8750-44d9-a3d3-6bf970b7f6d7",
   "metadata": {},
   "source": [
    "## 2. Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc3f43-6427-488f-ae33-84dd8d42f52a",
   "metadata": {},
   "source": [
    "Memilih hanya data ulasan yang memiliki kata 'tiket', maksimal 6000 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c257c5a-5dfe-4025-8014-2a8337031b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat list untuk menyimpan ulasan\n",
    "scraping_reviews = []\n",
    "\n",
    "# Mengambil 6000 ulasan yang mengandung kata 'tiket'\n",
    "for scraping in result:\n",
    "    if 'tiket' in scraping['content'].lower():\n",
    "        scraping_reviews.append(scraping)\n",
    "        if len(scraping_reviews) == 6000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd41581-a22c-479b-a2dd-14a284202955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_filter = pd.DataFrame(scraping_reviews)\n",
    "\n",
    "print(\"total data: \", len(df_filter))\n",
    "print(\"tipe data ulasan (content): \", df_filter['content'].dtypes)\n",
    "df_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa232f7-032d-47e8-9fbf-33b7dac1c1ab",
   "metadata": {},
   "source": [
    "## 3. Simpan Dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b0280-bc6d-4e93-8305-531f83b1f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter.to_csv(\"ulasan_tiket_kai_access.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cae228",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a98f8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data Checking :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f9378-322f-4a61-b06c-e31f70dd4831",
   "metadata": {},
   "source": [
    "Melakukan pengecekan data, seperti nilai null dan memisahkan fitur yang akan digunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b639fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ulasan_tiket_kai_access.csv')\n",
    "\n",
    "df_total = len(df)\n",
    "print(\"total data: \", df_total)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memisahkan kolom content sebagai fitur\n",
    "df = df.drop(columns=df.columns.difference(['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek fitur yang kosong\n",
    "value_null = df.content.isnull().sum()\n",
    "# cek fitur yang memiliki kata 'tiket'\n",
    "value_counts = df.content.str.contains('tiket', case=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336bebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"cek data null : \", value_null)\n",
    "print(\"jumlah ulasan/content yang memiliki kata 'tiket' : \", value_counts)\n",
    "\n",
    "print(\"total data:\", len(df))\n",
    "print(\"kolom:\", len(df.columns),\"|\", df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18e11e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data Cleaning :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d6b41",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Casefolding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24bb46",
   "metadata": {},
   "source": [
    "<i>Proses melakukan konversi teks. Mengubah huruf besar menjadi huruf kecil, dan mengubah huruf aksen ke bentuk tanpa aksen yang setara (mis: huruf Ã© menjadi e, huruf E menjadi e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a29eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def casefolding(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaning'] = df['content'].apply(casefolding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1371ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['cleaning']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4e1ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070fc123",
   "metadata": {},
   "source": [
    "<i>Proses membersihkan atau membuang noise (angka, tanda baca, emoji, multi spasi, dan baris enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleansing(text):\n",
    "    \n",
    "    # menghapus karakter yang bukan huruf, angka, atau spasi\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # menghapus angka menjadi satu spasi\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # menghapus multi-spasi menjadi satu spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaning'] = df['cleaning'].apply(cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['cleaning']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a751c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hasil cleaning\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4140fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data Normalize :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff2972",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95805b8",
   "metadata": {},
   "source": [
    "<i>Proses menemukan kata dasar dengan menghilangkan semua imbuhan yang menyatu pada kata. Misalnya kata \"diperbaiki\" akan diubah menjadi \"baik\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed27965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "    \n",
    "def stemming(text):\n",
    "    \n",
    "    stemmed_text = stemmer.stem(text)\n",
    "    return stemmed_text\n",
    "\n",
    "df['normalize'] = df['cleaning'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa991f-5bc5-439a-b2ba-f1ddecbf2c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['normalize']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf18706",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Slang Word Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc5957",
   "metadata": {},
   "source": [
    "<i>Proses mengubah kata non-baku (slang) menjadi kata baku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb48856",
   "metadata": {},
   "outputs": [],
   "source": [
    "kbba_dictionary = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/insomniagung/kamus_kbba/main/kbba.txt', \n",
    "    delimiter='\\t', names=['slang', 'formal'], header=None, encoding='utf-8')\n",
    "\n",
    "slang_dict = dict(zip(kbba_dictionary['slang'], kbba_dictionary['formal']))\n",
    "kbba_dictionary.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_slangword(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    normalized_words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    normalized_text = ' '.join(normalized_words)\n",
    "    return normalized_text\n",
    "\n",
    "df['normalize'] = df['normalize'].apply(convert_slangword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7411c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['normalize']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil normalize\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daaae0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Words Removal :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3b8bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a2270",
   "metadata": {},
   "source": [
    "<i>Proses menghapus seluruh kata yang dianggap tidak memiliki makna. Seperti kata hubung \"yang\", \"di\", \"dan\", \"dari\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14826b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.stopword import StopWord\n",
    "\n",
    "def remove_stopword(text):\n",
    "    stopword = StopWord()\n",
    "    text = stopword.remove_stopword(text)\n",
    "    return text\n",
    "\n",
    "df['removal'] = df['normalize'].apply(remove_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['removal']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c14b627",
   "metadata": {},
   "source": [
    "### 4.2 Unwanted Word Removal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5ee59",
   "metadata": {},
   "source": [
    "<i>Proses membuat dictionary kata-kata yang kurang dianggap bermakna secara manual, lalu menghapus kata yang sama dari ulasan. Kata yang dianggap tidak bermakna yaitu seperti nama bulan dalam kalender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_unwanted_words(text):\n",
    "    unwanted_words = {'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', \n",
    "                      'sep', 'oct', 'nov', 'dec', 'januari', 'februari', 'maret', \n",
    "                      'april', 'mei', 'juni', 'juli', 'agustus', 'september', \n",
    "                      'oktober', 'november', 'desember', 'gin'}\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_words = [word for word in word_tokens if word not in unwanted_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "df['removal'] = df['removal'].apply(remove_unwanted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['removal']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b071db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 Short Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d21c4b",
   "metadata": {},
   "source": [
    "<i>Proses menghapus kata apapun yang kurang dari 3 karakter. Seperti kata 'di'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text):\n",
    "    return ' '.join([word for word in text.split() if len(word) >= 3])\n",
    "\n",
    "df['removal'] = df['removal'].apply(remove_short_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['removal']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2483a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil words removal\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa5d0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Tokenizing  :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Split Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdd215",
   "metadata": {},
   "source": [
    "<i>Proses pemisahan kata pada tiap ulasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def tokenizing(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "df['tokenizing'] = df['removal'].apply(tokenizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd025ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tokenizing']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b1d45-be70-4f8f-befd-5ff9ca8c61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenizing'].to_excel(\"tokenizing_ulasan_tiket_kai_access.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3206f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5.2 Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864db9f",
   "metadata": {},
   "source": [
    "<i>Proses melakukan pelabelan (positif dan negatif) pada ulasan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a87662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary kata positif yang digunakan :\n",
    "df_positive = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/SadamMahendra/ID-NegPos/main/positive.txt', sep='\\t')\n",
    "list_positive = list(df_positive.iloc[::, 0])\n",
    "\n",
    "# Dictionary kata negatif yang digunakan :\n",
    "df_negative = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/SadamMahendra/ID-NegPos/main/negative.txt', sep='\\t')\n",
    "list_negative = list(df_negative.iloc[::, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame positive words\n",
    "df_positive_words = pd.DataFrame({'List Positive': list_positive})\n",
    "print(\"Positive : \", df_positive_words.shape[0], \"kata.\")\n",
    "\n",
    "# Membuat DataFrame negative words\n",
    "df_negative_words = pd.DataFrame({'List Negative': list_negative})\n",
    "print(\"Negative : \", df_negative_words.shape[0], \"kata.\")\n",
    "\n",
    "# Menggabungkan DataFrame positive dan negative\n",
    "df_dictionary = pd.concat([df_positive_words, df_negative_words], axis=1)\n",
    "\n",
    "# Menampilkan DataFrame dengan tabel positif di sebelah kiri dan tabel negatif di sebelah kanan\n",
    "df_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c4df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# menghitung kata-kata positif/negatif pada dictionary lalu menentukan sentimennya :\n",
    "def sentiment_analysis_dictionary_id(text):\n",
    "    score = 0\n",
    "    positive_words = []\n",
    "    negative_words = []\n",
    "    neutral_words = []\n",
    "\n",
    "    for word in text:\n",
    "        if (word in list_positive):\n",
    "            score += 1\n",
    "            positive_words.append(word)\n",
    "        if (word in list_negative):\n",
    "            score -= 1\n",
    "            negative_words.append(word)\n",
    "        if (word not in list_positive and word not in list_negative): \n",
    "            neutral_words.append(word)\n",
    "\n",
    "    polarity = ''\n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "\n",
    "    result = {'positif': positive_words,'negatif':negative_words,'neutral': neutral_words}\n",
    "    return score, polarity, result, positive_words, negative_words, neutral_words\n",
    "\n",
    "hasil = df['tokenizing'].apply(sentiment_analysis_dictionary_id)\n",
    "hasil = list(zip(*hasil))\n",
    "df['polarity_score'] = hasil[0]\n",
    "df['polarity'] = hasil[1]\n",
    "hasil_kata_positive = hasil[3]\n",
    "hasil_kata_negative = hasil[4]\n",
    "hasil_kata_neutral = hasil[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2008a-cbb6-4335-ad6e-ea5dc54ecec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEK NEUTRAL\n",
    "all_netral_words = [word for sublist in hasil_kata_neutral for word in sublist]\n",
    "netral_freq = pd.Series(all_netral_words).value_counts().reset_index().rename(columns={'index': 'Neutral Word', 0: 'Frequency'})\n",
    "topword_neutral = netral_freq.head()\n",
    "\n",
    "# HAPUS NETRAL\n",
    "df = df[df.polarity != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung hasil sentiment analysis\n",
    "print(\"jumlah: \", df['polarity'].value_counts().sum())\n",
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27579901-c7e7-4686-a1be-b26c013b50c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5.2.1 Top Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce5350-78dc-4f17-ad0b-b0b2fe0dfaa3",
   "metadata": {},
   "source": [
    "Merupakan kata teratas yang paling sering muncul di seluruh dokumen berdasarkan kata dari kamus dictionary positive negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523158c-da12-4a93-aa3c-dc4841c818e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_words(hasil_kata_positive, hasil_kata_negative):\n",
    "    all_positive_words = [word for sublist in hasil_kata_positive for word in sublist]\n",
    "    all_negative_words = [word for sublist in hasil_kata_negative for word in sublist]\n",
    "    positive_freq = pd.Series(all_positive_words).value_counts().reset_index().rename(columns={'index': 'Positive Word', 0: 'Frequency'})\n",
    "    negative_freq = pd.Series(all_negative_words).value_counts().reset_index().rename(columns={'index': 'Negative Word', 0: 'Frequency'})\n",
    "    topword_positive = positive_freq.head(20)\n",
    "    topword_negative = negative_freq.head(20)\n",
    "    return topword_positive, topword_negative\n",
    "        \n",
    "top_kata_positive, top_kata_negative = top_words(hasil_kata_positive, hasil_kata_negative)\n",
    "result3 = pd.DataFrame(top_kata_positive)\n",
    "result4 = pd.DataFrame(top_kata_negative)\n",
    "\n",
    "concate_result = pd.concat([result3, result4], axis=1)\n",
    "concate_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea053586-3551-4af3-a29d-3d7ad39d2fca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2.2 Pie Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49209ad7-e5cd-4878-a7eb-314adc8155f7",
   "metadata": {},
   "source": [
    "Proses melakukan visualisasi jumlah sentimen positive & negative menggunakan Pie Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be267228-1b1e-4f70-b73c-5ef31ac531e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_sub = df.loc[df.polarity.isin(['positive', 'negative'])]\n",
    "sizes = [count for count in df_sub.polarity.value_counts()]\n",
    "explode = (0.1, 0)\n",
    "total_sizes = sum(sizes)\n",
    "fig, ax = plt.subplots(figsize=(6, 6), facecolor='none')\n",
    "labels = ['Negative', 'Positive']\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "wedgeprops = {'width': 0.7, 'edgecolor': 'white', 'linewidth': 2}\n",
    "pie = ax.pie(x=sizes, labels=['', ''], colors=colors, explode=explode,\n",
    "    autopct=lambda pct: \"{:.1f}%\\n({:d})\".format(pct, int(pct / 100 * total_sizes)),\n",
    "    textprops={'fontsize': 9, 'color': 'black'}, shadow=True,\n",
    "    wedgeprops=wedgeprops)\n",
    "ax.legend(pie[0], labels, loc='center left', fontsize=10)\n",
    "ax.set_title(f\"Sentiment Analysis on KAI Access Reviews \\n(Total: {total_sizes} reviews)\", \n",
    "             fontsize=10, color='black', pad=4)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1d4bf-9650-4d15-a69c-8af2a8999f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2.3 Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff12a0-71c9-48d1-ad9c-4b0b14379800",
   "metadata": {},
   "source": [
    "<i>Proses menampilkan seluruh kata dalam sentimen pada Wordcloud. Jika kata semakin sering muncul, maka ditampilkan dengan ukuran yang lebih besar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08314712-7f74-4cc0-9d0d-3ff1d6468365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "positive_words = df[df.polarity == 'positive']['tokenizing'].apply(pd.Series).stack().tolist()\n",
    "positive_word_counts = Counter(positive_words)\n",
    "\n",
    "negative_words = df[df.polarity == 'negative']['tokenizing'].apply(pd.Series).stack().tolist()\n",
    "negative_word_counts = Counter(negative_words)\n",
    "\n",
    "mask_pos = np.array(Image.open(\"img/train_pos.jpg\"))\n",
    "mask_neg = np.array(Image.open(\"img/train_neg.jpg\"))\n",
    "\n",
    "positive_wordcloud = WordCloud(width=1000, height=800, mask=mask_pos, max_words=2000,\n",
    "                               background_color='black').generate_from_frequencies(positive_word_counts)\n",
    "\n",
    "negative_wordcloud = WordCloud(width=1000, height=800, mask=mask_neg, max_words=2000,\n",
    "                               background_color='black').generate_from_frequencies(negative_word_counts)\n",
    "\n",
    "figPos, axPos = plt.subplots(figsize=(9, 4))\n",
    "axPos.imshow(positive_wordcloud.recolor(color_func=ImageColorGenerator(mask_pos)), interpolation='bilinear')\n",
    "axPos.axis('off')\n",
    "plt.show(figPos)\n",
    "\n",
    "figNeg, axNeg = plt.subplots(figsize=(9, 4))\n",
    "axNeg.imshow(negative_wordcloud.recolor(color_func=ImageColorGenerator(mask_neg)), interpolation='bilinear')\n",
    "axNeg.axis('off')\n",
    "plt.show(figNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ec089",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.3 Pembobotan TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26303cfd",
   "metadata": {},
   "source": [
    "<i>Proses memberikan nilai bobot pada dokumen. Proses TF-IDF (Term Frequency-Inverse Document Frequency) tujuannya untuk mengetahui seberapa penting suatu kata dalam dokumen tersebut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e334dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = df.copy()\n",
    "df['tokenizing'] = df['tokenizing'].astype(str)\n",
    "tf_idf = TfidfVectorizer()\n",
    "review = df['tokenizing'].values.tolist()\n",
    "tf_idf_vector = tf_idf.fit(review)\n",
    "X = tf_idf_vector.transform(review)\n",
    "y = df['polarity']\n",
    "\n",
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59c8d68",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Modeling :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9770cf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.1 Pemisahan Data (Train & Test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef01e6",
   "metadata": {},
   "source": [
    "<i>Proses pemisahan data latih (train) & data uji (test). Data latih (train) ditetapkan 90%, dan data uji (test) sebanyak 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=0)\n",
    "all_data = len(y)\n",
    "data_train = len(y_train)\n",
    "data_test = len(y_test)\n",
    "vector = X_train.shape, X_test.shape\n",
    "\n",
    "print(\"Total Data : \", all_data)\n",
    "print(\"Total Data Train : \", data_train)\n",
    "print(\"Total Data Test : \", data_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a23f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2 Radom Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deee71d",
   "metadata": {},
   "source": [
    "<i>Pada proses ini, data yang telah dibagi akan dimodeling dengan Random Forest Classifier untuk mendapatkan akurasi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc_fit = rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "predict = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3051258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "akurasi = accuracy_score(y_pred, y_test) * 100\n",
    "akurasi_bulat = round(akurasi, 1)\n",
    "print(\"Random Forest Classifier Accuracy Score: \", akurasi_bulat, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf535fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Evaluasi Performa Model :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7daed",
   "metadata": {},
   "source": [
    "### 7.1 Classification Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665b866",
   "metadata": {},
   "source": [
    "<i>Proses menampilkan hasil kinerja model klasifikasi. Membantu dalam menganalisis dan memahami seberapa baik model dapat memprediksi label dengan benar. Jika semakin tinggi persentase Precision, Recall, dan F1-Score maka model sudah seimbang dan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc013e29-e2e1-44ac-9a18-929685fe8dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef4d9a",
   "metadata": {},
   "source": [
    "<i>Proses menampilkan Confusion Matrix dan Menghitung Akurasi Model. Confusion Matrix menyatakan jumlah data uji (test) yang benar dan salah diklasifikasi. Menghasilkan output True Positive, True Negative, False Positive, dan False Negative. Jika jumlah True (Positive & Negative) lebih banyak dari False (Positive & Negative), maka hasil data uji (test) dikatakan sudah baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91cb75-5500-4371-a63c-de740d0fa8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ConfusionMatrixDisplay(cm, display_labels=rfc.classes_).plot(ax=ax) \n",
    "plt.show(fig) \n",
    "\n",
    "print()\n",
    "TP = cm[0, 0]\n",
    "TN = cm[1, 1]\n",
    "FP = cm[1, 0]\n",
    "FN = cm[0, 1]\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "resultAccuracy = round(Accuracy, 3)*100\n",
    "\n",
    "equation = \"(TP + TN) / (TP + TN + FP + FN) = Accuracy\"\n",
    "calculate = f\"{TP} + {TN} / {TP} + {TN} + {FP} + {FN} = {resultAccuracy}\"\n",
    "\n",
    "df_cm = pd.DataFrame({\n",
    "    \"Value\": [TP, TN, FP, FN],\n",
    "    \"Label\": [\n",
    "        \"True Positive\", \"True Negative\", \"False Positive\", \"False Negative\"\n",
    "    ],\n",
    "    \"As\": [\"TP\", \"TN\", \"FP\", \"FN\"]\n",
    "})\n",
    "\n",
    "print(\"Equation Accuracy:\")\n",
    "print(equation)\n",
    "print()\n",
    "print(\"Calculate Accuracy:\")\n",
    "print(calculate)\n",
    "\n",
    "print()\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef76a9",
   "metadata": {},
   "source": [
    "<center>- Selesai. -</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
